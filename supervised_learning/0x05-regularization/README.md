# regualrization

## What is Regularization?
In general, regularization means to make things regular or acceptable. This is exactly why we use it for applied machine learning. The term ‘regularization’ refers to a set of techniques that regularizes learning from particular features for traditional algorithms or neurons in the case of neural network algorithms.

It normalizes and moderates weights attached to a feature or a neuron so that algorithms do not rely on just a few features or neurons to predict the result. This technique helps to avoid the problem of overfitting.

## Why do we need regularization?
The goal of our machine learning algorithm is to learn the data patterns and ignore the noise in the data set and to solve such cases.

## Different Regularization techniques in Deep Learning:
* L1 and L2 regularization
* Dropout
* Data augmentation
* Early stopping

## What i learned from this chapter  
- What is regularization? What is its purpose?
- What is are L1 and L2 regularization? What is the difference between the two methods?
- What is dropout?
- What is early stopping?
- What is data augmentation?
- How do you implement the above regularization methods in Numpy? Tensorflow?
- What are the pros and cons of the above regularization methods?

### Done by : Mouhamed Charfi